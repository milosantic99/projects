<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="UTF-8">
  <title>Activation function</title>
  <link rel="stylesheet" href="./activation-function-help.component.css">

</head>
<body>
<h1>Activation function types</h1>



<div id = "div1">
  
  <ul style="list-style-type:square;">
    <li><b>ReLU</b>
      <ul style="list-style-type:none;">
        <li>
          The Rectified Linear Unit is the most commonly used activation function in deep learning models. The function returns 0 if it receives any negative input, but for any positive value x it returns that value back.
          ReLU activation function is the most common function used for hidden layers.
        </li>
      </ul>
    </li>
    <li><b>Tanh</b>
      <ul style="list-style-type:none;">
        <li>
          The hyperbolic tangent activation function is also referred to simply as the Tanh (also “tanh” and “TanH“) function. It is very similar to the sigmoid activation function and even has the same S-shape. The function takes any real value as input and outputs values in the range -1.
        </li>
      </ul>
    </li>
    <li><b>Sigmoid</b>
      <ul style="list-style-type:none;">
        <li>
          Sigmoid Function acts as an activation function in machine learning which is used to add non-linearity in a machine learning model.
          The sigmoid activation function, also called the logistic function, is traditionally a very popular activation function for neural networks. The input to the function is transformed into a value between 0.0 and 1.0.
        </li>
      </ul>
    </li>
    <li><b>Linear</b>
      <ul style="list-style-type:none;">
        <li>. It is a simple straight line activation function where our function is directly proportional to the weighted sum of neurons or input.</li>
      </ul>
    </li>
    <li><b>Softmax</b>
      <ul style="list-style-type:none;">
        <li>The softmax function is used as the activation function in the output layer of neural network models that predict a multinomial probability distribution. That is, softmax is used as the activation function for multi-class classification problems where class membership is required on more than two class labels.</li>
      </ul>
    </li>
  </ul>

</div>

</body>
</html>