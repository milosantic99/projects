<!DOCTYPE html>
<html lang="en" >
<head>
  <meta charset="UTF-8">
  <title>Optimizer types</title>
  <link rel="stylesheet" href="./optimizer-type-help.component.css">

</head>
<body>
<h1>Optimizer types</h1>
<div id = "div1">
  
  <ul style="list-style-type:square;">
    <li><b>Adam</b><br><br>
      <ul style="list-style-type:none;">
        <li>
          Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models. Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.
        </li>
      </ul>
    </li>
    <br>
    <br>
    <br>
    <li><b>SGD</b><br><br>
      <ul style="list-style-type:none;">
        <li>
          An optimizer is a function or an algorithm that modifies the attributes of the neural network, such as weights and learning rate. Thus, it helps in reducing the overall loss and improve the accuracy 
          Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable).
        </li>
      </ul>
    </li>
  </ul>

</div>
</body>
</html>